{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbjAW3oulSD3"
      },
      "source": [
        "Amishi Gupta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2SDOiY7lRz8"
      },
      "source": [
        "23/CS/048"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7ddVN36lRxZ"
      },
      "source": [
        "EXP-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkduWSNvkn5v",
        "outputId": "61aae177-5237-42ec-f788-b99302a65c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APutBP7AldkS",
        "outputId": "70cee870-004c-4ad6-9d41-43117d319f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns: Index(['text'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/poems-100.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"Columns:\", df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TMJprJ5ldhu",
        "outputId": "671c11aa-7fd4-4234-c05a-7d91933aae3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "o my luve's like a red, red rose\n",
            "that’s newly sprung in june;\n",
            "o my luve's like the melodie\n",
            "that’s sweetly play'd in tune.\n",
            "\n",
            "as fair art thou, my bonnie lass,\n",
            "so deep in luve am i:\n",
            "and i will luve thee still, my dear,\n",
            "till a’ the seas gang dry:\n",
            "\n",
            "till a’ the seas gang dry, my dear,\n",
            "and the rocks melt wi’ the sun:\n",
            "i will luve thee still, my dear,\n",
            "while the sands o’ life shall run.\n",
            "\n",
            "and fare thee well, my only luve\n",
            "and fare thee well, a while!\n",
            "and i will come again, my luve,\n",
            "tho’ it were ten thousand\n"
          ]
        }
      ],
      "source": [
        "text = \" \".join(df[df.columns[0]].astype(str).tolist())\n",
        "text = text.lower()\n",
        "\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9ZMVuh0ldTn",
        "outputId": "dc1b70b0-94f5-4669-9525-54990b09ba2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Vocabulary size: 5441\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "def tokenize(text):\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    text = text.replace(\"\\n\", \" <eol> \")\n",
        "    return text.split()\n",
        "\n",
        "tokens = tokenize(text)\n",
        "\n",
        "vocab = sorted(set(tokens))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buwnisSeldQP",
        "outputId": "23cd1118-2b9c-42d3-bd60-1dc777a07bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sequences: 27997\n"
          ]
        }
      ],
      "source": [
        "seq_length = 5\n",
        "def create_sequences(tokens):\n",
        "    sequences = []\n",
        "    for i in range(len(tokens) - seq_length):\n",
        "        seq = tokens[i:i+seq_length]\n",
        "        target = tokens[i+seq_length]\n",
        "        sequences.append((seq, target))\n",
        "    return sequences\n",
        "\n",
        "data = create_sequences(tokens)\n",
        "print(\"Total sequences:\", len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X_XxkQUSldHr"
      },
      "outputs": [],
      "source": [
        "#one hot encoding\n",
        "def one_hot_encode(word):\n",
        "    vec = torch.zeros(vocab_size)\n",
        "    vec[word2idx[word]] = 1\n",
        "    return vec\n",
        "\n",
        "def prepare_onehot_batch(batch):\n",
        "    inputs, targets = [], []\n",
        "    for seq, target in batch:\n",
        "        seq_vec = [one_hot_encode(w) for w in seq]\n",
        "        inputs.append(torch.stack(seq_vec))\n",
        "        targets.append(word2idx[target])\n",
        "    return torch.stack(inputs), torch.tensor(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R0RqmY4HnFmH"
      },
      "outputs": [],
      "source": [
        "class RNN_OneHot(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Sx96DbDMnLHh"
      },
      "outputs": [],
      "source": [
        "class LSTM_OneHot(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c9fiBH1wnOm6"
      },
      "outputs": [],
      "source": [
        "def prepare_index_batch(batch):\n",
        "    inputs, targets = [], []\n",
        "    for seq, target in batch:\n",
        "        inputs.append([word2idx[w] for w in seq])\n",
        "        targets.append(word2idx[target])\n",
        "    return torch.tensor(inputs), torch.tensor(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Xa68rkwBnOkT"
      },
      "outputs": [],
      "source": [
        "class RNN_Embed(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XdRZ2_C4nOht"
      },
      "outputs": [],
      "source": [
        "class LSTM_Embed(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iz2R0aHgnOfH"
      },
      "outputs": [],
      "source": [
        "def train_model(model, prepare_fn, epochs=5, batch_size=64):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        random.shuffle(data)\n",
        "\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i+batch_size]\n",
        "            inputs, targets = prepare_fn(batch)\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            if prepare_fn == prepare_onehot_batch:\n",
        "                inputs = inputs.float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    print(\"Training Time:\", round(time.time() - start_time, 2), \"seconds\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq49v6jjnZv_",
        "outputId": "c20c246d-38d6-449c-f01d-33bc1ffb85ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RNN OneHot\n",
            "Epoch 1, Loss: 2938.5147\n",
            "Epoch 2, Loss: 2655.6715\n",
            "Epoch 3, Loss: 2408.8057\n",
            "Epoch 4, Loss: 2126.9864\n",
            "Epoch 5, Loss: 1804.9381\n",
            "Training Time: 113.0 seconds\n",
            "\n",
            "Training LSTM One Hot\n",
            "Epoch 1, Loss: 2831.4551\n",
            "Epoch 2, Loss: 2552.2154\n",
            "Epoch 3, Loss: 2306.5348\n",
            "Epoch 4, Loss: 1988.4147\n",
            "Epoch 5, Loss: 1590.1402\n",
            "Training Time: 496.5 seconds\n",
            "\n",
            "Training RNN Embedding\n",
            "Epoch 1, Loss: 2850.6638\n",
            "Epoch 2, Loss: 2493.5088\n",
            "Epoch 3, Loss: 2154.5649\n",
            "Epoch 4, Loss: 1813.6521\n",
            "Epoch 5, Loss: 1495.0357\n",
            "Training Time: 44.95 seconds\n",
            "\n",
            "Training LSTM Embedding\n",
            "Epoch 1, Loss: 2794.5537\n",
            "Epoch 2, Loss: 2424.5987\n",
            "Epoch 3, Loss: 2115.3855\n",
            "Epoch 4, Loss: 1767.1085\n",
            "Epoch 5, Loss: 1421.2855\n",
            "Training Time: 59.78 seconds\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 128\n",
        "embed_dim = 100\n",
        "\n",
        "print(\"\\nTraining RNN OneHot\")\n",
        "model1 = train_model(RNN_OneHot(vocab_size, hidden_size, vocab_size), prepare_onehot_batch)\n",
        "\n",
        "print(\"\\nTraining LSTM One Hot\")\n",
        "model2 = train_model(LSTM_OneHot(vocab_size, hidden_size, vocab_size), prepare_onehot_batch)\n",
        "\n",
        "print(\"\\nTraining RNN Embedding\")\n",
        "model3 = train_model(RNN_Embed(vocab_size, embed_dim, hidden_size), prepare_index_batch)\n",
        "\n",
        "print(\"\\nTraining LSTM Embedding\")\n",
        "model4 = train_model(LSTM_Embed(vocab_size, embed_dim, hidden_size), prepare_index_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UQaeHWYoqesV"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, seed_text, length=20, use_onehot=False):\n",
        "    model.eval()\n",
        "    words = seed_text.lower().split()\n",
        "\n",
        "    for _ in range(length):\n",
        "        seq = words[-seq_length:]\n",
        "\n",
        "        if use_onehot:\n",
        "            seq_vec = torch.stack([one_hot_encode(w) for w in seq])\n",
        "            seq_vec = seq_vec.unsqueeze(0).to(device)\n",
        "            output = model(seq_vec.float())\n",
        "        else:\n",
        "            seq_idx = torch.tensor([[word2idx[w] for w in seq]]).to(device)\n",
        "            output = model(seq_idx)\n",
        "\n",
        "        predicted = torch.argmax(output).item()\n",
        "        words.append(idx2word[predicted])\n",
        "\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8MD0G_Eqeo_",
        "outputId": "d0b3552a-e9ea-4a32-e38a-abadf0e6be92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RNN One Hot:\n",
            " the night was the house and the body or the young men and the sea of the sea of the earth of the\n",
            "\n",
            "LSTM One Hot:\n",
            " the night was he was his eyes <eol> the little systems that the bride is the air <eol> and this is the same\n",
            "\n",
            "RNN Embedding:\n",
            " the night was the <eol> texan ranch <eol> the cleanhaird yankee girl works and each other <eol> <eol> i do not love theeyet\n",
            "\n",
            "LSTM Embedding:\n",
            " the night was the same <eol> <eol> i am not a minute longer <eol> <eol> i am not a minute longer <eol> <eol>\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nRNN One Hot:\\n\", generate_text(model1, \"the night was\", 20, True))\n",
        "print(\"\\nLSTM One Hot:\\n\", generate_text(model2, \"the night was\", 20, True))\n",
        "print(\"\\nRNN Embedding:\\n\", generate_text(model3, \"the night was\", 20))\n",
        "print(\"\\nLSTM Embedding:\\n\", generate_text(model4, \"the night was\", 20))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
